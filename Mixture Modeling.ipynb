{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1f61ef",
   "metadata": {},
   "source": [
    "##### %%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01750553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Here All your Names, Surnames, and Student ID (Codice Persona)\n",
    "\n",
    "e.g.\n",
    "\n",
    "[\n",
    "    \"Mario\", \"Beraha\", \"123431451\",\n",
    "    \"Matteo\", \"Gianella\", \"913931\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa8f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750369a",
   "metadata": {},
   "source": [
    "# Mixture Modelling\n",
    "\n",
    "## aka modelling heterogeneity\n",
    "\n",
    "$\\newcommand{\\iid}{\\stackrel{\\mbox{iid}}{\\sim}}$\n",
    "\n",
    "\n",
    "In mixture model, we account for heterogeneity in the data by assuming that there is not \"one single\" data generating process, but actually K of them.\n",
    "\n",
    "Therefore, we expect to find K different subpopulations (== clusters) in our data, and that each subpopulation is homogeneous: it is suitably modeled by a density, typically from a parametric family.\n",
    "\n",
    "In its most general form, let $f_1(\\cdot), \\ldots, f_K(\\cdot)$ be $K$ probability density functions over a space $\\mathbb{Y}, \\mathcal{Y}$, $\\mathbf{w} = (w_1, \\ldots, w_K)$ a vector in the $K$-simplex; a mixture model assumes the following likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "    y_1, \\ldots, y_n \\mid \\mathbf{w}, \\{f_j\\}_{j=1}^K \\iid f^*(y) := \\sum_{j=1}^K w_j f_j(y)\n",
    "    \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "We refer to the $f_j$'s as the _components_ of the mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff5c81",
   "metadata": {},
   "source": [
    "$\\newcommand{\\ind}{\\stackrel{\\small \\mbox{ind}}{\\sim}}$\n",
    "\n",
    "The connection to clustering is made more explicit by introducing \"cluster assignment\" variables $c_i$, $i=1, \\ldots, n$ such that\n",
    "$$\n",
    "    P(c_i = h \\mid \\mathbf{w}) = w_h\n",
    "$$\n",
    "\n",
    "Then, the mixture likelihood can be restated as the following hierarchical model\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        y_i \\mid c_i = h, \\{f_j\\}_{j=1}^K & \\ind f_h \\\\\n",
    "        P(c_i = h \\mid \\mathbf{w}) = w_h\n",
    "    \\end{aligned}\n",
    "    \\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c363558",
   "metadata": {},
   "source": [
    "Usually, $\\{f_j\\}_{j=1}^K$ belong to the same parametric family (e.g., the normal distribution), and differ only for the specific values of the parameters (e.g., every component has a different mean/variance). But this might not always be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c57e89",
   "metadata": {},
   "source": [
    "# Exercise 1: Gaussian Mixture Model\n",
    "\n",
    "Consider the dataset in the file \"gestational_age_data.csv\", that contains the gestational age of more than two thousand children (in days).\n",
    "\n",
    "We can model the data using a mixture of three gaussian distributions.\n",
    "\n",
    "\\begin{equation*}\n",
    "    y_1, \\ldots, y_n \\mid \\mathbf{w}, \\mathbf{\\mu}, \\mathbf{\\sigma}^2 \\iid \\sum_{h=1}^5 w_h \\mathcal{N}(\\mu_h, \\sigma^2_h) \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "1.1) Describe a simple prior for $\\mathbf{w}$\n",
    "\n",
    "1.2) Using the likelihood in (2), derive the full conditional for $\\mathbf{w}$\n",
    "\n",
    "1.3) What is a simple semi-conjugate prior for $(\\mu_h, \\sigma_h^2)$. Derive the full conditional for these parameters\n",
    "\n",
    "1.4) Describe a Gibbs sampler that alternates between updating the $c_i$'s, the weights $\\mathbf w$ and the \"unique values\" $(\\mu_1, \\sigma_1^2), \\ldots (\\mu_K, \\sigma_K^2)$ where in this example $K=5$.\n",
    "\n",
    "\n",
    "1.5) Complete the model by specifying a prior such that the expected value of each $\\mu_h$ is equal to the empirical/sample mean and the expected value of each $\\sigma^2_h$ is equal to one half of the sample variance. Choose the remaining parameters \"wisely\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c75413",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "1.1)\n",
    "\n",
    "1.2)\n",
    "\n",
    "1.3)\n",
    "\n",
    "1.4)\n",
    "\n",
    "1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"songs_data.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.acousticness.to_numpy()\n",
    "np.random.shuffle(data)\n",
    "sns.histplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b609cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_gibbs(data, cluster_allocs, uniq_vals, weights):\n",
    "    \"\"\"\n",
    "    Runs one iteration of the gibbs sampler\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a numpy array of floats, contains the observations\n",
    "    cluster_allocs: a numpy array of integers, contains the c_i's\n",
    "        (same length of data)\n",
    "    uniq_vals: a numpy array of size [K, 2], contains (mean, variance)\n",
    "        for every component\n",
    "    weights: a numpy array of size [K], contains the weights of the\n",
    "        components\n",
    "    \"\"\"\n",
    "    \n",
    "    n_clus = len(weights)\n",
    "    \n",
    "    for h in range(n_clus):\n",
    "        clusdata = data[cluster_allocs == h]\n",
    "        if len(clusdata) == 0:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_prior()\n",
    "        else:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_fullcond(\n",
    "                clusdata, uniq_vals[h, 0], uniq_vals[h, 1])\n",
    "    \n",
    "    cluster_allocs = update_cluster_allocs(data, weights, uniq_vals)\n",
    "    \n",
    "    weights = update_weights(cluster_allocs, n_clus)\n",
    "    \n",
    "    return cluster_allocs, uniq_vals, weights\n",
    "\n",
    "\n",
    "\n",
    "def run_mcmc(data, niter=7500, nburn=2500, thin=5, n_clus=5):\n",
    "    cluster_allocs = tfd.Categorical(probs=np.ones(n_clus) / n_clus).sample(len(data))\n",
    "    weights = np.ones(n_clus) / n_clus\n",
    "    \n",
    "    # TODO Complete by sampling from an appropriate distribution\n",
    "    uniq_vals = np.dstack([\n",
    "        tfd.VARIABLE1(??, ??).sample(n_clus),\n",
    "        tfd.VARIABLE2(??, ??).sample(n_clus)])[0, :, :]\n",
    "    \n",
    "    allocs_out = []\n",
    "    uniq_vals_out = []\n",
    "    weights_out = []\n",
    "    \n",
    "    for i in range(niter):\n",
    "        cluster_allocs, uniq_vals, weights = run_one_gibbs(\n",
    "            data, cluster_allocs, uniq_vals, weights)\n",
    "        \n",
    "        if i > nburn and i % thin == 0:\n",
    "            allocs_out.append(cluster_allocs)\n",
    "            uniq_vals_out.append(uniq_vals)\n",
    "            weights_out.append(weights)\n",
    "            \n",
    "        if i % 10 == 0:\n",
    "            print(\"\\rIter {0} / {1}\".format(i+1, niter), flush=True, end=\" \")\n",
    "            \n",
    "    return allocs_out, uniq_vals_out, weights_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "def update_cluster_allocs(data, weights, uniq_vals):\n",
    "    # Hint: the update of each c_i can be done independently of\n",
    "    # the other c_j's (j != i) ---> Use numpy broadcasting /vectorizaiton\n",
    "    # to speed up the code\n",
    "    # Hint: you might need to evaluate the likelihood of each observation\n",
    "    # with parameters in each cluster. The following snippet returns a\n",
    "    # [n, K] matrix with [i, j]-entry equal to log_prob(y_i | \\mu_j, \\sigma^2_j)\n",
    "    \n",
    "    logprobs = tfd.Normal(\n",
    "        uniq_vals[:, 0], np.sqrt(uniq_vals[:, 1])).log_prob(data[:, np.newaxis])\n",
    "    \n",
    "    # TODO: probs should be a [n, n_clus] matrix with [i, j]-entry equal to\n",
    "    # P(c_i = j | all the rest)\n",
    "    probs = None\n",
    "    return tfd.Categorical(probs=probs).sample()\n",
    "\n",
    "\n",
    "def update_weights(cluster_allocs, n_clus):\n",
    "    # Hint: you might need to count how many observations are in each cluster\n",
    "    # you can use the following code\n",
    "    # n_by_clus = np.array([np.sum(cluster_allocs == h) for h in range(n_clus)])\n",
    "\n",
    "    \n",
    "    post_params = None\n",
    "    return tfd.Dirichlet(post_params.astype(float)).sample()\n",
    "\n",
    "\n",
    "def sample_uniq_vals_prior():\n",
    "    # TODO: choose appropriate variables and parameters\n",
    "    var = tfd.VARIABLE2(??, ??).sample()\n",
    "    mu = tfd.VARIABLE1(??, ??).sample()\n",
    "    return np.array([mu, var])\n",
    "    \n",
    "\n",
    "def sample_uniq_vals_fullcond(clusdata, mean_old, var_old):\n",
    "    # TODO: compute the full conditional parameters\n",
    "    var = tfd.VARIABLE2(??, ??).sample()\n",
    "    mu = tfd.VARIABLE1(??, ??).sample()\n",
    "    return np.array([mu, var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17404c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "allocs_out, uniq_vals_out, weights_out = run_mcmc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.1)\n",
    "for h in range(5):\n",
    "    currd = data[allocs_out[-1] == h]\n",
    "    plt.scatter(currd, np.zeros_like(currd) + 0.1 * (h+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90d89d",
   "metadata": {},
   "source": [
    "# Hint!!\n",
    "\n",
    "If you want to do Exs 1 & 2 in parallel, you can simulate a fake Markov Chain that mimics the output of Ex1.\n",
    "\n",
    "For $j=1, \\ldots, M$ sample\n",
    "\n",
    "$$\n",
    "    \\mu_1^{(j)} \\sim \\mathcal{N}(-5, 0.5), \\quad \\mu_2^{(j)} \\sim \\mathcal{N}(5, 0.5)\n",
    "$$\n",
    "$$\n",
    "    \\sigma^{2, (j)}_1 \\sim \\mathcal{IG}(3,3 ), \\quad \\sigma^{2, (j)}_2 \\sim \\mathcal{IG}(3,3 )\n",
    "$$\n",
    "$$\n",
    "    c_i^{(j)} \\sim \\text{Categorical(0.9, 0.1)}, \\text{for i =} 1, \\ldots, 50 \n",
    "$$\n",
    "$$\n",
    "    c_i^{(j)} \\sim \\text{Categorical(0.1, 0.9)}, \\text{for i =} 51, \\ldots, 100\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{w}^{(j)} \\sim \\text{Dirichlet}(5, 5)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2663fc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.325928 , -5.8873725, -5.2366805, -5.101785 , -4.7250714],\n",
       "       dtype=float32),\n",
       " array([5.00045  , 4.228856 , 5.431074 , 4.9407067, 5.2441854],\n",
       "       dtype=float32),\n",
       " array([1.1361796 , 5.7812686 , 0.36698982, 1.4152927 , 0.7762139 ],\n",
       "       dtype=float32),\n",
       " array([2.0479698 , 0.88734555, 1.3755735 , 1.0936158 , 1.1971056 ],\n",
       "       dtype=float32),\n",
       " array([[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "         1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32),\n",
       " array([[0.5240666 , 0.47593352],\n",
       "        [0.6517981 , 0.34820193],\n",
       "        [0.6613846 , 0.33861548],\n",
       "        [0.6872162 , 0.31278387],\n",
       "        [0.49321043, 0.50678945]], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 5\n",
    "N_mezzi = 50\n",
    "N = 2*N_mezzi\n",
    "\n",
    "dist_mu_1 = tfd.Normal(loc=-5,scale=0.5)\n",
    "dist_mu_2 = tfd.Normal(loc=5,scale=0.5)\n",
    "mu_1_vec = dist_mu_1.sample(M)\n",
    "mu_2_vec = dist_mu_2.sample(M)\n",
    "\n",
    "dist_sigma_2_1 = tfd.InverseGamma(concentration = 3, scale = 3)\n",
    "dist_sigma_2_2 = tfd.InverseGamma(concentration = 3, scale = 3)\n",
    "sigma_2_1_vec = dist_sigma_2_1.sample(M)\n",
    "sigma_2_2_vec = dist_sigma_2_2.sample(M)\n",
    "\n",
    "\n",
    "c_i_j = np.concatenate((np.reshape(tfd.Categorical(probs=[0.9,0.1]).sample(N_mezzi*M),(M,N_mezzi)), np.reshape(tfd.Categorical(probs=[0.1,0.9]).sample(N_mezzi*M),(M,N_mezzi))), axis=1)\n",
    "\n",
    "w_vec = tfd.Dirichlet([5,5]).sample(M)\n",
    "\n",
    "# i dati sono\n",
    "mu_1_vec, mu_2_vec, sigma_2_1_vec, sigma_2_2_vec, c_i_j, w_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8082580",
   "metadata": {},
   "source": [
    "# Exercise 2: Posterior inference in Mixture Models\n",
    "\n",
    "Mixture models are subject to the so called \"label switching\", which causes non-identifiability for all the parameters.\n",
    "\n",
    "Label switching means that permuting the \"labels\" {1, 2, ..., K} of the cluster yields the same exact likelihood.\n",
    "Hence, it does not make sense to talk about \"first\", \"second\" and \"third\" cluster.\n",
    "Moreover, also interpretation of the cluster parameters is tricky: we cannot say that\n",
    "\n",
    "$$\n",
    "    \\frac{1}{M} \\sum_{j=1}^M \\mu^{(j)}_1\n",
    "$$\n",
    "\n",
    "is an estimate of parameter $\\mu_1$. Also, it does not make sense to take averages of the cluster allocation labels.\n",
    "\n",
    "There are several ways to solve this non-identifiability issue, we will focus here on a decision-theoretic approach.\n",
    "\n",
    "\n",
    "2.1) Implement a function that finds the clustering, among the ones visited in the MCMC chain, that minimizes Binder's loss function:\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^n \\sum_{j=1}^n (\\mathbb I [c_i = c_j] - p_{ij})^2\n",
    "$$\n",
    "where $p_{ij}$ is the posterior probability that observations $i$ and $j$ belong to the same cluster.\n",
    "\n",
    "Note that $p_{ij}$ can be easily estimated from the MCMC chains!\n",
    "\n",
    "2.2) Given the estimated clustering $\\mathbf{c}^*$, estimate the parameters in each cluster. Let $c^*_h$ denote the index-set of the $h$--th cluster (e.g., $c^*_1 = \\{1, 3, 10\\}$, $c^*_2 = \\{0, 2, 4, 5, 6\\}$ ...)\n",
    "Then\n",
    "$$\n",
    "    \\hat \\mu_h = \\frac{1}{M} \\sum_{j=1}^M \\frac{1}{\\# c^*_h} \\sum_{i \\in c^*_h} \\mu^{(j)}_{c_i}\n",
    "$$\n",
    "where $\\mu^{(j)}_\\ell$  is the mean parameter in the $\\ell$--th cluster at the $j$--th MCMC iteration.\n",
    "\n",
    "Similarly for the variance $\\hat \\sigma_h^2$.\n",
    "\n",
    "2.3) Comment on the posterior inference obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c58e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psm(clus_alloc_chain):\n",
    "    \"\"\"\n",
    "    Returns the posterior similarity matrix, i.e.\n",
    "        out[i, j] = P(c_i == c_j | all the rest)\n",
    "    for each pair of observations\n",
    "    \"\"\"\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    out = np.zeros((c_chain.shape[1], c_chain.shape[1]))\n",
    "    # TODO: fill out!       \n",
    "    return out\n",
    "\n",
    "\n",
    "def minbinder_sample(clus_alloc_chain, psm):\n",
    "    \"\"\"\n",
    "    Finds the iteration for which the Binder loss is minimized\n",
    "    and returns the associated clustering\n",
    "    \"\"\"\n",
    "    losses = np.zeros(len(clus_alloc_chain))\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    \n",
    "    # You can either cycle through the iterations, or \n",
    "    # cycle through the entries in the PSM [i, j]\n",
    "    # and vectorize the same operation for each iteration!\n",
    "    \n",
    "    # TODO: compute the losses!\n",
    "   \n",
    "    best_iter = np.argmin(losses)\n",
    "    return clus_alloc_chain[best_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbee04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "psm = get_psm(allocs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebddda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clus = minbinder_sample(allocs_out, psm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.3)\n",
    "for h in range(5):\n",
    "    currd = data[best_clus == h]\n",
    "    plt.scatter(currd, np.zeros_like(currd) + 0.1 * (h+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals_given_clus(unique_vals_chain, clus_alloc_chain, best_clus):\n",
    "    c_allocs = np.stack(clus_alloc_chain)\n",
    "    uniq_vals = np.stack(unique_vals_chain)\n",
    "    means = uniq_vals[:, :, 0]\n",
    "    variances = uniq_vals[:, :, 1]\n",
    "    out = []\n",
    "    \n",
    "    for h in np.unique(best_clus):\n",
    "        data_idx = np.where(best_clus == h)[0]\n",
    "        uniq_vals_idx = c_allocs[:, data_idx] # -> Matrix [n_iter x n_data_in_clus]\n",
    "        means_by_iter = np.empty((c_allocs.shape[0], len(data_idx)))\n",
    "        vars_by_iter = np.empty_like(means_by_iter)\n",
    "        for i in range(c_allocs.shape[0]):\n",
    "            means_by_iter[i, :] = ???\n",
    "            vars_by_iter[i, :] = ???\n",
    "\n",
    "        avg_mean_by_iter = ???\n",
    "        avg_var_by_iter = ???\n",
    "        \n",
    "        muhat = np.mean(avg_mean_by_iter)\n",
    "        sigsqhat = np.mean(avg_var_by_iter)\n",
    "        out.append(np.array([muhat, sigsqhat]))\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40397b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_vals = unique_vals_given_clus(uniq_vals_out, allocs_out, best_clus)\n",
    "clus_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aad11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.sum(best_clus == np.arange(5)[:, np.newaxis], axis=1) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.1, bins=30)\n",
    "xx = np.linspace(np.min(data), np.max(data), 1000)\n",
    "out_dens = np.zeros_like(xx)\n",
    "for i in range(5):\n",
    "    comp_dens = tfd.Normal(clus_vals[i][0], np.sqrt(clus_vals[i][1])).prob(xx)\n",
    "    out_dens += weights[i] * comp_dens\n",
    "    plt.fill_between(xx, np.zeros_like(xx), weights[i] * comp_dens, alpha=0.6)\n",
    "\n",
    "plt.plot(xx, out_dens, c=\"black\", linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e985661",
   "metadata": {},
   "source": [
    "## Comment on the inference obtained\n",
    "\n",
    "The three clusters differ in...\n",
    "\n",
    "Their weights are ...\n",
    "\n",
    "Hence we can conclude that the population ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9f91c",
   "metadata": {},
   "source": [
    "# Exercise 3: Latent Dirichlet Allocation\n",
    "\n",
    "We now move to one of the most fundamental papers in Machine Learning / Text Mining: Latent Dirichlet Allocations by Blei et al (2003).\n",
    "\n",
    "The goal is topic modelling given a corpus (= collection of documents)\n",
    "\n",
    "Denote with $\\mathbf y_i = \\{y_{i1}, \\ldots, y_{i N_i}\\}$ the $i$--th document where each $y_{ij}$ denotes a word in a vocabulary, i.e. it is a categorical variable. Let $|V|$ be the size of the vocabulary (number of unique words) so that $y_{ij} \\in \\{1, 2, \\ldots, |V|\\}$, and $K$ the number of topics.\n",
    "\n",
    "For each topic $k$, we assume a different distribution over the vocabulary $\\beta_k = (\\beta_{k,1}, \\ldots \\beta_{k, |V|})$, where the vector $\\beta_k$ in the $|V|$-dimensional simplex encodes the probability of occurence of each word in topic $k$ (i.e., informally, $\\beta_{k, j} = P(\\text{\"we use word j when talking about topic k\"})$  \n",
    "\n",
    "Moreover, in each document more than one topic can be discussed. Denote with $w_i = (w_{i, 1}, \\ldots, w_{i, K})$ the weight of each topic in the $i$--th document.\n",
    "\n",
    "Then LDA assumes\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    P(y_{i, j} = v \\mid \\{\\beta_k\\}_{k=1}^J, w_i) &= \\sum_{k=1}^K w_{i, k} \\beta_{k, v}, \\quad v=1, \\ldots, |V| \\\\\n",
    "    \\beta_k &\\sim \\text{Dirichlet}(\\alpha_1, \\ldots, \\alpha_{|V|}) \\\\\n",
    "    w_i & \\sim \\text{Dirichlet}(0.5, \\ldots, 0.5)\n",
    "\\end{aligned}\n",
    "\\label{eq:LDA} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "3.1) Rewrite the likelihood introducing suitable auxiliary variables $t_{ij}$ so that $w_i$ does appear on the right hand side of the first equation in \\eqref{eq:LDA}\n",
    "\n",
    "3.2) Specify the hyperparameters of the prior for the $\\beta_j$'s such that, a priori, the expected value of $\\beta_{j, v}$ is proportional to the number of times word $v$ appears in the corpus. What about the variance?\n",
    "\n",
    "3.3) Implement a Gibbs sampler to perform posterior inference. Instead of identifying the mixture parameters, consider only the last iteration of the MCMC. What can we say about the topics? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee017678",
   "metadata": {},
   "source": [
    "### SOLUTIONS\n",
    "\n",
    "3.1) We introduce $t_{ij} \\sim \\text{TODO}$ (one for each $y_{ij}$) so that\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    P(y_{ij} = v \\mid \\{\\beta_k\\}, t_{ij} = h) &= \\beta_{h, v} \\\\\n",
    "    P(t_{ij} = h \\mid \\pi_i) &= \\text{TODO}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "3.2) We fix the expected value as ....\n",
    "\n",
    "To control the variance, call $d_v =  \\sum_{i=1}^D \\sum_{j=1}^{N_i} \\mathbb{I}[y_{ij} = v]$, so that $\\alpha_v = \\kappa d_v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469a57a",
   "metadata": {},
   "source": [
    "3.3) The joint model is\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\mathcal{L}(\\{y_{ij}\\}, \\{\\beta_k\\}, \\{w_i\\}, \\{t_{ij}\\}) &= \\prod_{i=1}^D \\prod_{j=1}^{N_i} w_{i, t_{ij}} \\prod_{v=1}^{|V|} \\beta_{t_{ij}, v}^{\\mathbb{I}[y_{ij} = v]} \\\\\n",
    "    & \\times \\prod_{k=1}^K \\frac{1}{B(\\mathbf \\alpha)} \\prod_{v=1}^{|V|} \\beta_{k, v}^{\\alpha_v - 1} \\mathbb{I}[\\beta_k \\in S^{|V|}] \\\\\n",
    "    & \\times \\prod_{i=1}^D \\frac{1}{B(\\mathbf{0.5})} \\prod_{k=1}^{K} w_{i, k}^{0.5 - 1} \\mathbb{I}[w_i \\in S^{K}]\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a26a0",
   "metadata": {},
   "source": [
    "Then it is easy to see that\n",
    "\n",
    "1)\n",
    "$$\n",
    "    \\mathcal{L}(w_i \\mid \\cdots) \\propto \\prod_{k=1}^{K} \\text{TODO}\n",
    "$$\n",
    "\n",
    "Hint: rewrite \n",
    "\n",
    "$$\n",
    "    \\prod_{j=1}^{N_i} w_{i, t_{ij}} = \\prod_{k=1}^K w_{i, k}^{n_{i, k}}\n",
    "$$\n",
    "where $n_{i, k} = ???$.\n",
    "\n",
    "Finally, conclude that $w_i \\mid \\cdots \\sim \\text{TODO}$\n",
    "\n",
    "\n",
    "2) $$ \\mathcal{L}(\\beta_k \\mid \\cdots) \\propto \\text{TODO}\n",
    "   $$\n",
    "   \n",
    "   Hence $\\beta_k \\mid \\cdots \\sim \\text{TODO}$\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "3) If $y_{ij} = v^*$, \n",
    "$$\n",
    "    P(t_{ij} = k \\mid \\cdots) \\propto w_{ik} \\beta_{k, v^*}, \\quad k=1, \\ldots, K\n",
    "$$\n",
    "\n",
    "Hence, $t_{ij}$ is categorically-distributed with probabilities proportional to $w_{ik} \\beta_{k, v^*}$. We can sample $t_{ij}$ from a Categorical distribution after having computed the unnormalized probabilities for each $k$ and having normalized them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install scikit-learn\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d46f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some preprocessing.\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"Removes digits and weird punctuation\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\__', '', text)\n",
    "    text = re.sub(r'\\___', '', text)\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=preprocess_text,\n",
    "    stop_words=\"english\",\n",
    "    strip_accents=\"unicode\",\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=500)\n",
    "\n",
    "vectorizer.fit(data)\n",
    "vectorizer.get_feature_names()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = vectorizer.build_analyzer()\n",
    "docs = []\n",
    "for doc in data:\n",
    "    tokens = analyzer(doc)\n",
    "    idxs = np.array(list(filter(\n",
    "        lambda x: x, \n",
    "        [vectorizer.vocabulary_.get(tok, None) for tok in tokens])))\n",
    "    docs.append(idxs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd468a1",
   "metadata": {},
   "source": [
    "## Docs is your final dataset, work only with it!\n",
    "\n",
    "docs is a list where every entry represents a document. A document is represented as a list of indexes (each work represents a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "voc_len = len(vectorizer.get_feature_names())\n",
    "alpha = TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gibbs_sampler(docs):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: a list of np.arrays of integers, each represnts the \n",
    "        words in a document\n",
    "        \n",
    "    word_topics are the t_{ij}'s\n",
    "    voc_weights are the \\beta_k's\n",
    "    doc_weights are the w_i's\n",
    "    \"\"\"\n",
    "    \n",
    "    word_topics = [\n",
    "        tfd.Categorical(probs=np.ones(n_topics)).sample(len(x)) for x in docs]\n",
    "    voc_weights = tfd.Dirichlet(alpha).sample(n_topics)\n",
    "    doc_weights = tfd.Dirichlet(np.ones(n_topics) * 0.5).sample(len(docs))\n",
    "    \n",
    "    for i in range(300):\n",
    "        print(\"\\r{0} / {1}\".format(i+1, 300), flush=True, end=\" \")\n",
    "        word_topics, doc_weights, voc_weights = run_one_lda(\n",
    "            docs, word_topics, doc_weights, voc_weights)\n",
    "    return word_topics, doc_weights, voc_weights\n",
    "\n",
    "\n",
    "def run_one_lda(docs, word_topics, doc_weights, voc_weights):    \n",
    "    word_topics = update_word_topics(doc_weights, voc_weights, docs)\n",
    "    doc_weights = update_doc_weights(word_topics)\n",
    "    voc_weights = update_voc_weights(docs, word_topics)\n",
    "    return word_topics, doc_weights, voc_weights\n",
    "\n",
    "\n",
    "def update_voc_weights(docs, word_topics):\n",
    "    \"\"\"Updates \\beta_k for every k\"\"\"\n",
    "    out = np.zeros((n_topics, voc_len))\n",
    "    for k in range(n_topics):\n",
    "        # Get all the words whose topic is k (i.e. t_ij = k)\n",
    "        # Kept words is a np.array of integers with the indexes/words\n",
    "        # of all y_{ij} for which t_{ij} = k\n",
    "        mask = [np.where(x == k) for x in word_topics]\n",
    "        kept_words = np.concatenate([x[y] for x, y in zip(docs, mask)])\n",
    "        \n",
    "        # Returns the unique values in kept_words and their counts\n",
    "        # For instance if kept_words = [1, 1, 1, 0, 5, 4]\n",
    "        # --> idxs = [0, 1, 4, 5]\n",
    "        # --> counts = [1, 3, 1, 1]\n",
    "        idxs, counts = np.unique(kept_words, return_counts=True)\n",
    "        \n",
    "        dir_param = ???\n",
    "        out[k] = tfd.DISTRIBUTION(dir_param).sample()\n",
    "    return out\n",
    "\n",
    "\n",
    "def update_doc_weights(word_topics):\n",
    "    \"\"\"Updates w_i for every i\"\"\"\n",
    "    out = np.zeros((len(docs), n_topics))\n",
    "    for doc_ind, word_topic_in_doc in enumerate(word_topics):\n",
    "        # word_topic_in_doc is the doc_ind-th row of word_topics\n",
    "        \n",
    "        # cnt returns how many times each topic appears in each document\n",
    "        # i.e. cnt[k] is the number of words ind the doc_ind-th document\n",
    "        # that have topic k\n",
    "        cnt = np.sum(\n",
    "            word_topic_in_doc == np.arange(n_topics)[:, np.newaxis], axis=1)\n",
    "        \n",
    "        dir_param = ???\n",
    "        out[doc_ind] = tfd.DISTRIBUTION(dir_param).sample()\n",
    "    return out\n",
    "\n",
    "\n",
    "def update_word_topics(doc_weights, voc_weights, docs):\n",
    "    out = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        # computes an [num_words x K] matrix with entries\n",
    "        # probas[l, m] beta_{m, v_l} * w_{i, m}\n",
    "        # where v_l is the word in position 'l'\n",
    "        # --> for each word (by row) represents the unnormalized probability\n",
    "        #     of each topic\n",
    "        probas = doc_weights[i][np.newaxis, :] * voc_weights[:, doc].T\n",
    "        \n",
    "        # normalize it approriately\n",
    "        probas = ???\n",
    "        out.append(tfd.Categorical(probs=probas).sample())\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topics, doc_weights, voc_weights = lda_gibbs_sampler(docs[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(n_topics):\n",
    "    top_ind = np.argpartition(voc_weights[k, :], -10)[-10:]\n",
    "    print(\"Topic: {0}\".format(k))\n",
    "    print(\", \".join([vectorizer.get_feature_names()[ind] for ind in top_ind]))\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
